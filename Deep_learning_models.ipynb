{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi!\n",
    "\n",
    "Here is my attempt to solve the eluvio movie scene segmentation challenge.\n",
    "\n",
    "Given a list of 64 movies, the goal is to predict the scene transition boundary between every two shot of a movie\n",
    "\n",
    "I approached the problem in 2 ways\n",
    "\n",
    "1. I tried to replicate the codebase provided by the original authors by converting the gvien data into the same format.\n",
    "However this led to a memory allocation error and I could not proceed with the method thereafter.\n",
    "\n",
    "2. With the given data, I tried to combine all the features such that I will be able to apply machine learning and deep learning models.\n",
    "\n",
    "\n",
    "I will be going through both these methods in the notebook\n",
    "\n",
    "I am using 37 movies for training and 27 movies for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tt0052357.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the given data looks like:\n",
    "\n",
    "We have:\n",
    "1. place feature, representing the features of images in the shot \n",
    "2. cast feature, data of people present in the shot with object detection models such as faster RCNN\n",
    "3. action feature, representing the action going with temporal convolutional networks \n",
    "4. audio feature. the audio of the shot and scene\n",
    "5. scene_transition_boundary_ground_truth, ground truth\n",
    "6. shot_end_frame, representing the boundary\n",
    "7. scene_transition_boundary_prediction, given predictions\n",
    "8. imdb_id, movie id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place': tensor([[6.1556e-02, 1.0177e-01, 5.2085e-02,  ..., 6.7148e-02, 1.5965e-03,\n",
       "          9.5967e-03],\n",
       "         [5.2110e-02, 1.3152e-01, 4.7641e-02,  ..., 7.6565e-02, 1.4197e-03,\n",
       "          1.1087e-02],\n",
       "         [8.7748e-02, 2.4885e+00, 5.4332e-02,  ..., 7.1745e-01, 8.1299e-01,\n",
       "          3.3096e-01],\n",
       "         ...,\n",
       "         [1.2349e-01, 4.4957e-01, 2.4556e-01,  ..., 1.0015e-02, 1.1907e-01,\n",
       "          5.1556e-03],\n",
       "         [1.9621e-02, 7.2868e-01, 2.4324e-01,  ..., 7.4836e-03, 3.7045e-02,\n",
       "          7.4661e-02],\n",
       "         [1.2028e-01, 2.9355e-01, 7.4574e-02,  ..., 4.0226e-02, 7.1149e-02,\n",
       "          1.5697e-01]]),\n",
       " 'cast': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0073,  0.1230, -0.0396],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0309,  0.1172, -0.0647],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0290,  0.0804,  0.0003],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'action': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 5.5101, 0.0000,  ..., 0.0000, 0.0132, 3.0242],\n",
       "         ...,\n",
       "         [0.0000, 5.7000, 0.0000,  ..., 0.0000, 0.0000, 0.1954],\n",
       "         [0.0000, 4.5412, 0.0000,  ..., 0.0000, 0.1389, 1.3127],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " 'audio': tensor([[-0.0993, -0.0711, -0.0708,  ..., -0.0829, -0.0911, -0.1051],\n",
       "         [-0.0943, -0.0590, -0.0602,  ..., -0.0682, -0.0753, -0.0831],\n",
       "         [-0.0442, -0.0324, -0.0318,  ..., -0.0332, -0.0386, -0.0454],\n",
       "         ...,\n",
       "         [ 0.0216, -0.0060, -0.0127,  ..., -0.0135, -0.0496, -0.0513],\n",
       "         [-0.0800, -0.0120, -0.0191,  ..., -0.0272, -0.0592, -0.0654],\n",
       "         [-0.1281, -0.0604, -0.0645,  ..., -0.0776, -0.0705, -0.0774]]),\n",
       " 'scene_transition_boundary_ground_truth': tensor([False, False, False,  ..., False, False,  True]),\n",
       " 'shot_end_frame': tensor([    45,   1149,   1437,  ..., 183782, 184354, 184794],\n",
       "        dtype=torch.int32),\n",
       " 'scene_transition_boundary_prediction': tensor([0.0000, 0.0000, 0.4878,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        dtype=torch.float16),\n",
       " 'imdb_id': 'tt0052357'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we represent the baseline provided to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('data2'):\n",
    "    with open('data2/'+str(i), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        fav = {'scene_transition_boundary_ground_truth': data['scene_transition_boundary_ground_truth'], 'scene_transition_boundary_prediction': data['scene_transition_boundary_prediction'],\n",
    "                        'shot_end_frame': data['shot_end_frame'], 'imdb_id': data['imdb_id']}\n",
    "        pickle.dump(fav, open('data_dir2/'+str(data['imdb_id'])+'.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of IMDB IDs: 27\n",
      "Scores: {\n",
      "    \"AP\": 0.4615802116187682,\n",
      "    \"mAP\": 0.48199051394228226,\n",
      "    \"Miou\": 0.46508357436009745,\n",
      "    \"Precision\": 0.2929881877369351,\n",
      "    \"Recall\": 0.7506495769377108,\n",
      "    \"F1\": 0.4117560720605304\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python evaluate_sceneseg.py data_dir2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first approach where I convert the data into the format of the authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a list of all the movies from the json file available at' https://drive.google.com/drive/folders/1F-uqCKnhtSdQKcDUiL3dRcLOrAxHargz' \n",
    "I did this so that I could retrieve the json in the exact format need for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the json file and storing all the keys present. These keys are the imdb_ids\n",
    "with open('/home/arjun/eluvio/SceneSeg/data/scene318/meta/scene_movie318.json') as f:\n",
    "    rem = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_new={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the imdb id that we have in our data\n",
    "arr_names=[]\n",
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    arr_names.append(i[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1101, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tt0107290',\n",
       " 'tt0110604',\n",
       " 'tt0052357',\n",
       " 'tt0114746',\n",
       " 'tt0945513',\n",
       " 'tt0178868',\n",
       " 'tt0976051',\n",
       " 'tt0117060',\n",
       " 'tt1001508',\n",
       " 'tt0379786',\n",
       " 'tt2488496',\n",
       " 'tt0822832',\n",
       " 'tt0092099',\n",
       " 'tt0440963',\n",
       " 'tt1099212',\n",
       " 'tt0086190',\n",
       " 'tt0163025',\n",
       " 'tt2024544',\n",
       " 'tt0217505',\n",
       " 'tt0123755',\n",
       " 'tt0172495',\n",
       " 'tt0113277',\n",
       " 'tt0088247',\n",
       " 'tt0115759',\n",
       " 'tt1119646',\n",
       " 'tt0137523',\n",
       " 'tt0479884',\n",
       " 'tt0361748',\n",
       " 'tt0118715',\n",
       " 'tt0253474',\n",
       " 'tt1375666',\n",
       " 'tt1205489',\n",
       " 'tt0124315',\n",
       " 'tt0063442',\n",
       " 'tt0088944',\n",
       " 'tt0096320',\n",
       " 'tt0119488',\n",
       " 'tt0108399',\n",
       " 'tt0078788',\n",
       " 'tt1707386',\n",
       " 'tt0103855',\n",
       " 'tt0100405',\n",
       " 'tt0082089',\n",
       " 'tt0120689',\n",
       " 'tt0319061',\n",
       " 'tt0068646',\n",
       " 'tt0073195',\n",
       " 'tt0103776',\n",
       " 'tt0099423',\n",
       " 'tt0443272',\n",
       " 'tt1038919',\n",
       " 'tt0409459',\n",
       " 'tt0116922',\n",
       " 'tt2582846',\n",
       " 'tt0112573',\n",
       " 'tt0190332',\n",
       " 'tt0075314',\n",
       " 'tt0120890',\n",
       " 'tt0281358',\n",
       " 'tt1412386',\n",
       " 'tt0399201',\n",
       " 'tt0120382',\n",
       " 'tt0116282',\n",
       " 'tt0780571']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_names #The list we were provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tt0107290'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/home/arjun/Downloads/data')[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking only those keys in our data and saving in a json file. This is the shots per scene and start frame and end frame of the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rem.keys():\n",
    "    if i in arr_names:\n",
    "        json_new[i] = rem[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('json_new.json', 'w') as f:\n",
    "    json.dump(json_new, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the codebase provided by 'https://anyirao.com/projects/SceneSeg.html'\n",
    "I first convert the data provided in the challenge to match the format needed by the model\n",
    "\n",
    "For action features:\n",
    "    We require a pickle file where each shot is a key and the value is a array representing the action in the shot\n",
    "We have 64 pickle files each representing shots in the movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It will look something like this\n",
    "\n",
    "#{  '0000': []   ,  '0001':[]   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    with open('/home/arjun/Downloads/data/'+i, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "\n",
    "    x = data['action'].detach().cpu().numpy()\n",
    "    dict_store = {}\n",
    "    count=0\n",
    "    for j in x:\n",
    "        dict_store[str(count).zfill(4)] = j\n",
    "        count+=1\n",
    "    output = open('/home/arjun/eluvio/SceneSeg/data/scene318/act_feat/'+str(i[:-4])+'.pkl', 'wb')\n",
    "    pickle.dump(dict_store, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For label features: I pciked up the labels from 'https://drive.google.com/open?id=1F-uqCKnhtSdQKcDUiL3dRcLOrAxHargz' which is a part of the codebase.\n",
    "I picked up those files which are present in the training data given to us.\n",
    "We have 64 labels, 1 for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    if i[:-4] in arr_names:\n",
    "        shutil.copy('/home/arjun/eluvio/SceneSeg/data/scene318/shot_movie318/'+str(i[:-4])+'.txt', '/home/arjun/eluvio/SceneSeg/data/scene318/shot_movie318_2/')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeated the same for the shot_id. We now have the shot for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    if i[:-4] in arr_names:\n",
    "        shutil.copy('/home/arjun/eluvio/SceneSeg/data/scene318/label318/'+str(i[:-4])+'.txt', '/home/arjun/eluvio/SceneSeg/data/scene318/labels/')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cast features:\n",
    "    We require the cast of each movie to be stored in pickle files\n",
    "       Hence 64 pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    with open('/home/arjun/Downloads/data/'+i, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "\n",
    "    x = data['cast'].detach().cpu().numpy()\n",
    "    dict_store = {}\n",
    "    for j in x:\n",
    "        dict_store[str(count).zfill(4)] = j\n",
    "        count+=1\n",
    "    output = open('/home/arjun/eluvio/SceneSeg/data/scene318/cast_feat/'+str(i[:-4])+'.pkl', 'wb')\n",
    "    pickle.dump(dict_store, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Places and audio have the same format \n",
    "We have a folder for each movie and we save a numpy file for each shot of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    count=0\n",
    "    with open('/home/arjun/Downloads/data/'+i, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for j in data['place']:\n",
    "        x = (j.detach().cpu().numpy())\n",
    "        try:\n",
    "            os.mkdir('/home/arjun/eluvio/SceneSeg/data/scene318/place_feat_att/'+str(i[:-4]))\n",
    "            \n",
    "            with open('/home/arjun/eluvio/SceneSeg/data/scene318/place_feat_att/'+str(i[:-4])+ '/shot_'+str(count).zfill(4)+'.npy', 'wb') as g:\n",
    "                np.save(g,x)\n",
    "            count+=1\n",
    "        except:\n",
    "            with open('/home/arjun/eluvio/SceneSeg/data/scene318/place_feat_att/'+str(i[:-4])+ '/shot_'+str(count).zfill(4)+'.npy', 'wb') as g:\n",
    "                np.save(g,x)\n",
    "            count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('/home/arjun/Downloads/data'):\n",
    "    count=0\n",
    "    with open('/home/arjun/Downloads/data/'+i, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    for j in data['audio']:\n",
    "        x = j.detach().cpu().numpy()\n",
    "        try:\n",
    "            os.mkdir('/home/arjun/eluvio/SceneSeg/data/scene318/aud_feat_att/'+str(i[:-4]))\n",
    "            \n",
    "            with open('/home/arjun/eluvio/SceneSeg/data/scene318/aud_feat_att/'+str(i[:-4])+ '/shot_'+str(count).zfill(4)+'.npy', 'wb') as g:\n",
    "                np.save(g,x)\n",
    "            count+=1\n",
    "        except:\n",
    "            with open('/home/arjun/eluvio/SceneSeg/data/scene318/aud_feat_att/'+str(i[:-4])+ '/shot_'+str(count).zfill(4)+'.npy', 'wb') as g:\n",
    "                np.save(g,x)\n",
    "            count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once we do this and run the codebase with the command  'python run.py config/all.py'. Unfortunately, I encountered a CUDA memeory error. \n",
    "# I tried to sample for 2 movies instead of 64 and I received the same error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I then decided that recreating with limited resources is not a viable solution and I had to try simpler models first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the second step where I use the data in a more efficient manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclf = LinearSVC()\\nprint(X.shape)\\nclf.fit(X,Y)\\nprint(2)\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from focal_loss import BinaryFocalLoss\n",
    "\n",
    "# get all files in the folder\n",
    "def feature(directory,test=False,clf=None):\n",
    "    '''\n",
    "    In this function we first extract all the features: places,audio, action, cast.\n",
    "    The next step is to stack these features in a horizontal manner\n",
    "    Once that is done we stack every 2 features since we need to predict the transition \n",
    "    This leads to same size for ground truth and the labels\n",
    "    '''\n",
    "    files = os.listdir(directory)\n",
    "    total_files = len(files)  # Calculate total number of files\n",
    "\n",
    "    j = 0\n",
    "    for i in range(total_files):\n",
    "        filename = directory + '/' + files[i]\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        feat1 = data['place']\n",
    "        feat1 = feat1.data.numpy()  # convert tensors into numpy arrays for sklearn\n",
    "        feat1_size = feat1.shape[1]\n",
    "\n",
    "        feat2 = data['cast']\n",
    "        feat2 = feat2.data.numpy()\n",
    "        feat2_size = feat2.shape[1]\n",
    "\n",
    "        feat3 = data['action']\n",
    "        feat3 = feat3.data.numpy()\n",
    "        feat3_size = feat3.shape[1]\n",
    "\n",
    "        feat4 = data['audio']\n",
    "        feat4 = feat4.data.numpy()\n",
    "        feat4_size = feat4.shape[1]\n",
    "\n",
    "        x = np.hstack((feat1, feat2, feat3, feat4))\n",
    "        y = data['scene_transition_boundary_ground_truth']\n",
    "\n",
    "        scaler = preprocessing.StandardScaler().fit(x)\n",
    "        x_scaled = scaler.transform(x)\n",
    "\n",
    "        # Fold the data set to obtain features from adjoining shots\n",
    "        #The shape is 512*3 + 2048 = 3584. \n",
    "        N = x.shape[0]  \n",
    "        x_fold = np.zeros((N - 1, 2 * x.shape[1]))  # increasing by 2*x since we need to hstack again\n",
    "        #Upon folding shape is 3584* = 7168. This is for the 2nd dimension. Shape of 1st dimension depends on number of movies\n",
    "        for l in range(N - 1):\n",
    "            x_fold[l, :] = np.hstack((x_scaled[l, :], x_scaled[l + 1, :]))  # combining every 2 shots\n",
    "        if test is True:\n",
    "                predicted = model(torch.tensor(x_fold, dtype=torch.float32))\n",
    "                print(x_fold.shape, 'Test')\n",
    "                fav = {'scene_transition_boundary_ground_truth': y, 'scene_transition_boundary_prediction': predicted.detach().numpy(),\n",
    "                        'shot_end_frame': data['shot_end_frame'], 'imdb_id': data['imdb_id']}\n",
    "                pickle.dump(fav, open('data_dir/'+str(data['imdb_id'])+'.pkl', \"wb\"))\n",
    "                \n",
    "        else:\n",
    "            if (j == 0):\n",
    "                X = x_fold\n",
    "                Y = y\n",
    "            else:\n",
    "                X = np.concatenate((X, x_fold), axis=0)\n",
    "                Y = np.concatenate((Y, y),axis=0)\n",
    "\n",
    "            j = j + 1\n",
    "    if test is False:\n",
    "        return X,Y\n",
    "\n",
    "directory = 'data'\n",
    "X,Y = feature(directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class dataset(Dataset):\n",
    "    ''\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "trainset = dataset(X, Y)  # DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    A simple linear network\n",
    "    '''\n",
    "    def __init__(self,input_shape):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,64)\n",
    "        self.b1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.b2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.b1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.b2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "#hyper parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "# Model , Optimizer, Loss\n",
    "model = Net(input_shape=X.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "accur = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\tloss : 0.20708124339580536\n",
      "epoch 2\tloss : 0.1191529706120491\n",
      "epoch 4\tloss : 0.05605647340416908\n",
      "epoch 6\tloss : 0.02156488411128521\n",
      "epoch 8\tloss : 0.01382701937109232\n",
      "epoch 10\tloss : 0.007840287871658802\n",
      "epoch 12\tloss : 0.004932770039886236\n",
      "epoch 14\tloss : 0.0026233962271362543\n",
      "epoch 16\tloss : 0.0019705926533788443\n",
      "epoch 18\tloss : 0.0014836674090474844\n",
      "epoch 20\tloss : 0.0010893383296206594\n",
      "epoch 22\tloss : 0.0009672963060438633\n",
      "epoch 24\tloss : 0.0008987343171611428\n",
      "epoch 26\tloss : 0.0007681747083552182\n",
      "epoch 28\tloss : 0.0007364962948486209\n",
      "epoch 30\tloss : 0.0006202621734701097\n",
      "epoch 32\tloss : 0.0006228536949492991\n",
      "epoch 34\tloss : 0.0005413030739873648\n",
      "epoch 36\tloss : 0.0004977845237590373\n",
      "epoch 38\tloss : 0.00045849519665353\n",
      "epoch 40\tloss : 0.00045264113578014076\n",
      "epoch 42\tloss : 0.0004228444304317236\n",
      "epoch 44\tloss : 0.0003862351586576551\n",
      "epoch 46\tloss : 0.0004019600455649197\n",
      "epoch 48\tloss : 0.00036098083364777267\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    for j, (x_train, y_train) in enumerate(trainloader):\n",
    "        # calculate output\n",
    "        output = model(x_train)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(output, y_train.reshape(-1, 1))\n",
    "\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % 2 == 0:\n",
    "        losses.append(loss)\n",
    "        print(\"epoch {}\\tloss : {}\".format(i, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(losses) # plotting by columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.image import GramianAngularField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GramianAngularField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1645, 7168) Test\n",
      "(1804, 7168) Test\n",
      "(1211, 7168) Test\n",
      "(1188, 7168) Test\n",
      "(1837, 7168) Test\n",
      "(1447, 7168) Test\n",
      "(2134, 7168) Test\n",
      "(2710, 7168) Test\n",
      "(2765, 7168) Test\n",
      "(2760, 7168) Test\n",
      "(918, 7168) Test\n",
      "(1827, 7168) Test\n",
      "(1364, 7168) Test\n",
      "(1549, 7168) Test\n",
      "(1442, 7168) Test\n",
      "(1287, 7168) Test\n",
      "(1194, 7168) Test\n",
      "(2012, 7168) Test\n",
      "(1763, 7168) Test\n",
      "(1805, 7168) Test\n",
      "(1380, 7168) Test\n",
      "(2609, 7168) Test\n",
      "(1487, 7168) Test\n",
      "(2548, 7168) Test\n",
      "(727, 7168) Test\n",
      "(2395, 7168) Test\n",
      "(1873, 7168) Test\n"
     ]
    }
   ],
   "source": [
    "feature('data2/',True,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the model with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of IMDB IDs: 27\n",
      "Scores: {\n",
      "    \"AP\": 0.21574664494479046,\n",
      "    \"mAP\": 0.2331933218420753,\n",
      "    \"Miou\": 0.3836303020554841,\n",
      "    \"Precision\": 142.22222222222223,\n",
      "    \"Recall\": 78.51851851851852,\n",
      "    \"F1\": 99.19740677212857\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# for standard scaler \n",
    "!python evaluate_sceneseg.py data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of IMDB IDs: 27\n",
      "Scores: {\n",
      "    \"AP\": 0.2452689693226328,\n",
      "    \"mAP\": 0.26102519658286005,\n",
      "    \"Miou\": 0.38804374907272776,\n",
      "    \"Precision\": 142.22222222222223,\n",
      "    \"Recall\": 73.0,\n",
      "    \"F1\": 94.54120948486168\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# for standard scaler \n",
    "!python evaluate_sceneseg.py data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of IMDB IDs: 27\n",
      "Scores: {\n",
      "    \"AP\": 0.277382974194029,\n",
      "    \"mAP\": 0.2955761839920259,\n",
      "    \"Miou\": 0.346955531988229,\n",
      "    \"Precision\": 142.22222222222223,\n",
      "    \"Recall\": 31.40740740740741,\n",
      "    \"F1\": 48.64512523899404\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# for minmax scaler\n",
    "!python evaluate_sceneseg.py data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7168,)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58296,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim, hidden_dim, vocab_size, label_size, batch_size = 32,64,37,len(Y),32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
