{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TF info\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from focal_loss import BinaryFocalLoss\n",
    "\n",
    "# get all files in the folder\n",
    "def feature(directory,test=False,clf=None):\n",
    "    '''\n",
    "    In this function we first extract all the features: places,audio, action, cast.\n",
    "    The next step is to stack these features in a horizontal manner\n",
    "    Once that is done we stack every 2 features since we need to predict the transition \n",
    "    This leads to same size for ground truth and the labels\n",
    "    '''\n",
    "    files = os.listdir(directory)\n",
    "    total_files = len(files)  # Calculate total number of files\n",
    "\n",
    "    j = 0\n",
    "    for i in range(total_files):\n",
    "        filename = directory + '/' + files[i]\n",
    "        f = open(filename, 'rb')\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        feat1 = data['place']\n",
    "        feat1 = feat1.data.numpy()  # convert tensors into numpy arrays for sklearn\n",
    "        feat1_size = feat1.shape[1]\n",
    "\n",
    "        feat2 = data['cast']\n",
    "        feat2 = feat2.data.numpy()\n",
    "        feat2_size = feat2.shape[1]\n",
    "\n",
    "        feat3 = data['action']\n",
    "        feat3 = feat3.data.numpy()\n",
    "        feat3_size = feat3.shape[1]\n",
    "\n",
    "        feat4 = data['audio']\n",
    "        feat4 = feat4.data.numpy()\n",
    "        feat4_size = feat4.shape[1]\n",
    "\n",
    "        x = np.hstack((feat1, feat2, feat3, feat4))\n",
    "        y = data['scene_transition_boundary_ground_truth']\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler().fit(x)\n",
    "        x_scaled = scaler.transform(x)\n",
    "\n",
    "        # Fold the data set to obtain features from adjoining shots\n",
    "        N = x.shape[0]  # changed from x_scaled\n",
    "        x_fold = np.zeros((N - 1, 2 * x.shape[1]))  # increasing by 2*x since we need to stack\n",
    "        for l in range(N - 1):\n",
    "            x_fold[l, :] = np.hstack((x_scaled[l, :], x_scaled[l + 1, :]))  # combining every 2 shots\n",
    "        if test is True:\n",
    "                print(x_fold.shape, 'Test')\n",
    "                fav = {'scene_transition_boundary_ground_truth': y, 'scene_transition_boundary_prediction': (clf.predict(x_fold)),\n",
    "                        'shot_end_frame': data['shot_end_frame'], 'imdb_id': data['imdb_id']}\n",
    "                pickle.dump(fav, open('data_dir/'+str(data['imdb_id'])+'.pkl', \"wb\"))\n",
    "        else:\n",
    "            if (j == 0):\n",
    "                X = x_fold\n",
    "                Y = y\n",
    "            else:\n",
    "                X = np.concatenate((X, x_fold), axis=0)\n",
    "                Y = np.concatenate((Y, y),axis=0)\n",
    "\n",
    "            j = j + 1\n",
    "    if test is False:\n",
    "        \n",
    "        return X,Y\n",
    "\n",
    "\n",
    "\n",
    "# code by Tae Hwan Jung(Jeff Jung) @graykode\n",
    "# Reference : https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = 'data'\n",
    "X,Y = feature(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of IMDB IDs: 27\n",
      "Scores: {\n",
      "    \"AP\": 0.09184545653521352,\n",
      "    \"mAP\": 0.09697054551742584,\n",
      "    \"Miou\": 0.2900415666153362,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluate_sceneseg.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  fscore_dict[imdb_id] = 2 * p * r / (p + r)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"Precision\": 0.19795936594406763,\n",
      "    \"Recall\": 0.15172587742132168,\n",
      "    \"F1\": NaN\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python evaluate_sceneseg.py data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-449db9cf2715>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data2/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-def74f336422>\u001b[0m in \u001b[0;36mfeature\u001b[1;34m(directory, test, clf)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mx_fold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# combining every 2 shots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 fav = {'scene_transition_boundary_ground_truth': y, 'scene_transition_boundary_prediction': (clf.predict(x_fold)),\n\u001b[0m\u001b[0;32m     54\u001b[0m                         'shot_end_frame': data['shot_end_frame'], 'imdb_id': data['imdb_id']}\n\u001b[0;32m     55\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_dir/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imdb_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "feature('data2/',True,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58296, 7168)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = RandomForestClassifier(min_samples_leaf=3)\n",
    "print(X.shape)\n",
    "clf.fit(X,Y)\n",
    "print(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature('data2/',True,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_sceneseg.py data_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
